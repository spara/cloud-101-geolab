{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "279c3db6",
   "metadata": {},
   "source": [
    "# SAGE GAGE 2025 SIG Demo:  \n",
    "![](https://www.earthscope.org/app/uploads/2024/11/2025_conference_banner-1024x333.webp)\n",
    "### GeoLab processing EarthScope GNSS $ARCO_{*1}$ data in $S3_{*2}$\n",
    "\n",
    "*1  ARCO =  Analysis ready, cloud optimized\n",
    "\n",
    "*2 S3 is AWS object storage (simple storage service)\n",
    "\n",
    "Goal:\n",
    "Demonstrate multiple methods for processing GNSS observation data in the SAGE/GAGE GeoLab in AWS\n",
    "\n",
    "Comparisons:\n",
    "1. Serial download and process: in GeoLab\n",
    "2. Serial read from S3 and process: in GeoLab\n",
    "3. Parallel read and process using Dask-Gateway in GeoLab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfffee1-14ce-42ab-a0d7-1b07fce3beb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -y tiledb\n",
    "!pip install tiledb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e017bf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3 \n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import tiledb\n",
    "import os\n",
    "\n",
    "from dask.distributed import Client\n",
    "from dask.distributed import LocalCluster\n",
    "import dask, dask.array\n",
    "from dask_gateway import Gateway\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import requests\n",
    "from pathlib import Path\n",
    " \n",
    "from earthscope_sdk.auth.device_code_flow import DeviceCodeFlowSimple\n",
    "from earthscope_sdk.auth.auth_flow import NoTokensError\n",
    "\n",
    "def unix_time_millis(dt):\n",
    "    epoch = datetime.utcfromtimestamp(0)\n",
    "    return int((dt - epoch).total_seconds() * 1e3)\n",
    "\n",
    "def utc_time(dt):\n",
    "    epoch = datetime.utcfromtimestamp(0)\n",
    "    return timedelta(seconds=dt*1e-3) + epoch\n",
    "\n",
    "def get_es_file(url, directory_to_save_file='./', token_path='./'):\n",
    "    \"\"\"function to get earthscope data using es-sdk\n",
    "    modified from https://gitlab.com/earthscope/public/earthscope-sdk\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : string\n",
    "        url of desired file at gage-data.earthscope.org\n",
    "    directory_to_save_file : str, optional\n",
    "        path of directory in which to save the file, by default cwd\n",
    "    token_path : str, optional\n",
    "        path of directory in which to save the token, by default './'\n",
    "    \"\"\"\n",
    "    # instantiate the device code flow subclass\n",
    "    device_flow = DeviceCodeFlowSimple(Path(token_path))\n",
    "    try:\n",
    "      # get access token from local path\n",
    "      device_flow.get_access_token_refresh_if_necessary()\n",
    "    except NoTokensError:\n",
    "      # if no token was found locally, do the device code flow\n",
    "      device_flow.do_flow()\n",
    "    token = device_flow.access_token\n",
    "\n",
    "    # request a file and provide the token in the Authorization header\n",
    "    file_name = Path(url).name\n",
    "\n",
    "    r = requests.get(url, headers={\"authorization\": f\"Bearer {token}\"})\n",
    "    if r.status_code == requests.codes.ok:\n",
    "      # save the file\n",
    "      with open(Path(Path(directory_to_save_file) / file_name), 'wb') as f:\n",
    "          for data in r:\n",
    "              f.write(data)\n",
    "    else:\n",
    "      #problem occured\n",
    "      print(f\"failure: {r.status_code}, {r.reason}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22726d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install georinex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703912b9",
   "metadata": {},
   "source": [
    "## 1. Serial download RINEX and process 'Traditional Method'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d53537e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import georinex as gr #https://github.com/geospace-code/georinex/blob/main/Readme_OBS.md\n",
    "import os\n",
    "\n",
    "directory_path = \"./rinex_data\"\n",
    "os.makedirs(directory_path, exist_ok=True)\n",
    "\n",
    "dl_time=[]\n",
    "pr_time=[]\n",
    "snr_li=[]\n",
    "date_li=[]\n",
    "year=2024\n",
    "\n",
    "start_t = time.time()\n",
    "for doy in np.arange(1,10):\n",
    "    #download\n",
    "    url='https://gage-data.earthscope.org/archive/gnss/rinex/obs/2024/%03d/p057%03d0.24d.Z' %(doy,doy)\n",
    "    print('downloading: ', url)\n",
    "    get_es_file(url, 'rinex_data')\n",
    "    \n",
    "    #process\n",
    "    fn='rinex_data/p057%03d0.24d.Z' %doy\n",
    "    # Use Georinex to convert Rinex _> Xarray Dataframe\n",
    "    obs = gr.load(fn, use='G', meas=['S1'])\n",
    "    g=obs['S1'].mean().values\n",
    "    snr_li+=[g]\n",
    "    date_li+=[datetime(year, 1, 1) + timedelta(days=int(doy - 1))]\n",
    "    #delete the file\n",
    "    os.remove(fn)\n",
    "    \n",
    "end_t = time.time()\n",
    "dl_time=end_t-start_t\n",
    "\n",
    "snr_rnx=np.mean(snr_li)\n",
    "\n",
    "# Plot results ##########\n",
    "plt.plot(date_li, snr_li)\n",
    "plt.xticks(rotation=45)\n",
    "plt.axhline(snr_rnx)\n",
    "plt.title('%s Days of L1C SNR Average  by reading from S3 in daily chunks \\n took %.1f seconds to download and process' %(len(date_li),dl_time))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb96b30",
   "metadata": {},
   "source": [
    "## What is this doing?\n",
    "\n",
    "1. Downloading GNSS Observations\n",
    "![](https://customersupport.septentrio.com/servlet/rtaImage?eid=ka04y000000CyrT&feoid=00Nf3000002xZ3u&refid=0EMf3000000qixY)\n",
    "\n",
    "(credit: Septentrio)\n",
    "\n",
    "2. Community bespoke python decoder: GeoRinex\n",
    "   \n",
    "![](../images/gr.png)\n",
    "\n",
    "3. Using Xarray to slice/average\n",
    "   \n",
    "![](https://docs.xarray.dev/en/stable/_static/Xarray_Logo_RGB_Final.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513af9fb",
   "metadata": {},
   "source": [
    "# ARCO GNSS Objects:  Read from TileDB in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d26709",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!aws sso login --profile es-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797ec7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "session=boto3.Session(profile_name='es-dev')\n",
    "credentials=session.get_credentials()\n",
    "credentials=credentials.get_frozen_credentials()\n",
    "\n",
    "tdb_config={\"vfs.s3.region\": \"us-east-2\", \"sm.io_concurrency_level\": 12, \n",
    "                                              \"vfs.s3.aws_access_key_id\":credentials.access_key, \"vfs.s3.aws_secret_access_key\":credentials.secret_key,\n",
    "                                             \"vfs.s3.aws_session_token\":credentials.token}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b382efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pnum='P057'  #using datasources-api https://datasources-api.dev.earthscope.org/docs#/Stream/Stream-find_streams\n",
    "# edid=\"01GVDXYKWXCFP2N5WAJN4CX36F\"\n",
    "# year=2024\n",
    "# start_time='2024-05-11'\n",
    "# duration='12' #hours  \n",
    "# constell= 0 #'GPS'\n",
    "# obs_code=12611 #L1C   int(12611).to_bytes(2, 'big').decode(\"utf-8\")\n",
    "\n",
    "# bucket='s3://repository-stage-us-east-2-mlmoghi3ooss/gnss/obs/tiledb/'\n",
    "\n",
    "pnum = 'P057'  \n",
    "edid = \"01GVDXYKWXCFP2N5WAJN4CX36F\"\n",
    "year = 2024\n",
    "start_time = '2024-05-11'\n",
    "duration = '12'  # hours  \n",
    "constell = 0  # 'GPS'\n",
    "obs_code = 12611  # 1C  \n",
    "# Convert the observation code to a byte string representation (e.g., '12611' -> b'12611')\n",
    "obs_code_str = int(obs_code).to_bytes(2, 'big').decode(\"utf-8\")\n",
    "\n",
    "\n",
    "bucket = 's3://repository-stage-us-east-2-mlmoghi3ooss/geolab-gnss-demo/'\n",
    "\n",
    "uri=bucket+pnum+\".tdb\"\n",
    "\n",
    "start_date=datetime.fromisoformat(start_time)\n",
    "durationz=timedelta(hours=int(duration)) #1hour\n",
    "end_date=start_date+durationz\n",
    "\n",
    "# Print the values to verify\n",
    "print(f\"URI: {uri}\")\n",
    "print(f\"Start Date: {start_date}\")\n",
    "print(f\"End Date: {end_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef327c39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# With pandas\n",
    "with tiledb.open(uri, mode=\"r\", config=tdb_config,) as A:\n",
    "    A.upgrade_version()\n",
    "    # note that array indexes are half-open like NumPy\n",
    "    # df = A.df[unix_time_millis(start_date): unix_time_millis(end_date),constell,:,obs_code]\n",
    "    df = A.df[unix_time_millis(start_date): unix_time_millis(end_date),:] \n",
    "\n",
    "df[\"constell\"] = constell\n",
    "df[\"obs_code\"] = obs_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6496a124",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab069fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.sat==1].snr.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72045cea",
   "metadata": {},
   "source": [
    "# 2. Serial read from S3 and process: in GeoLab\n",
    "\n",
    "Our second strategy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0d2746",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "start_t = time.time()\n",
    "\n",
    "snr_li=[]\n",
    "date_li=[]\n",
    "for doy in np.arange(1,51):\n",
    "    start=datetime(year, 1, 1) + timedelta(days=int(doy - 1))\n",
    "    date_li+=[start]\n",
    "    end=start+timedelta(days=1)\n",
    "    start=unix_time_millis(start)\n",
    "    end=unix_time_millis(end)\n",
    "  \n",
    "    with tiledb.open(uri,mode=\"r\", config=tdb_config,) as A:\n",
    "        df=A.df[slice(int(start), int(end)),:]['snr']\n",
    "        snr_li+=[df.mean()]\n",
    "        \n",
    "for_loop=np.nanmean(np.array(snr_li))\n",
    "\n",
    "end_t = time.time()\n",
    "for_loop_t=end_t - start_t\n",
    "\n",
    "# Plot results\n",
    "plt.plot(date_li, snr_li)\n",
    "plt.axhline(for_loop)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('%s Days of L1C SNR Average \\n by reading from S3 in daily chunks took %.1f seconds' %(len(date_li),for_loop_t))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d376298",
   "metadata": {},
   "source": [
    "![](../images/ar.png)\n",
    "credit: R. Abernathy/ Pangeo Project\n",
    "\n",
    "https://speakerdeck.com/rabernat/beyond-fair-what-data-infrastructure-does-open-science-need?slide=33"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0207ea5",
   "metadata": {},
   "source": [
    "![](../images/co.png)\n",
    "credit: R. Abernathy/ Pangeo Project\n",
    "\n",
    "https://speakerdeck.com/rabernat/beyond-fair-what-data-infrastructure-does-open-science-need?slide=33"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04381544",
   "metadata": {},
   "source": [
    "# 3. Parallel read and process using Dask in GeoLab\n",
    "\n",
    "our final comparison demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444c2841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gateway = Gateway()  # Uses values configured for the 2i2c Dask hub (recommended)\n",
    "\n",
    "# options = gateway.cluster_options()\n",
    "# options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab04e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and scale gateway cluster\n",
    "# cluster = gateway.new_cluster(options)\n",
    "# cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e56848",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Connect to the gateway cluster\n",
    "# client = cluster.get_client()\n",
    "# client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587be916-6ccf-4078-968b-0b5adfa75941",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b92e94-b697-425c-aa24-206db862d028",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = Client()\n",
    "client\n",
    "\n",
    "# Display the Dask dashboard link\n",
    "print(\"Dask dashboard available at:\", client.dashboard_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b297f08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get schema and Non-empty domain of Tdb object:\n",
    "with tiledb.open(uri,mode=\"r\", config=tdb_config,) as A:\n",
    "    ned=A.nonempty_domain()\n",
    "    # print(A.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a962fa81",
   "metadata": {},
   "source": [
    "# What is this doing?\n",
    "\n",
    "![](https://docs.2i2c.org/_images/scalable_research_hub.png)\n",
    "\n",
    "Credit: 2i2c\n",
    "\n",
    "https://docs.2i2c.org/about/distributions/research/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa260897",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(start)\n",
    "start_t = time.time()\n",
    "# This produces an array slice\n",
    "def slice_tiledb(path, slc):\n",
    "    with tiledb.open(uri,mode=\"r\", config=tdb_config,) as A:\n",
    "        return A[slc,0,:,12611]['snr']\n",
    "        # return A[slc,:]['snr']\n",
    "       \n",
    "\n",
    "# Partition the array into delayed slices\n",
    "chunk = 43200000*4 # tile size?\n",
    "delayed_slices = list(\n",
    "    dask.delayed(slice_tiledb)(uri, slice(int(start), int(start+chunk))) for \n",
    "                               start in \n",
    "                               np.arange(ned[0][0],ned[0][1]-chunk,step=chunk)) #-86400*1e3\n",
    "\n",
    "# This creates a Dask array from the delayed slices\n",
    "darray = dask.array.concatenate(\n",
    "    dask.array.from_delayed(x,\n",
    "                            shape=(chunk,), dtype=np.float64)\n",
    "                            for x in delayed_slices)\n",
    "\n",
    "#Everything up until here is lazy - nothing is really computed\n",
    "\n",
    "# This triggers the entire computation\n",
    "# print(darray.compute_chunk_sizes())\n",
    "darray.persist()\n",
    "gw_cluster = darray.mean().compute()\n",
    "\n",
    "end_t = time.time()\n",
    "gw_cluster_t=end_t - start_t\n",
    "\n",
    "print('Object start date: ',utc_time(ned[0][0]))\n",
    "print('Object end date: ',utc_time(ned[0][1]))\n",
    "\n",
    "days_p=(ned[0][1]-ned[0][0])/(86400e3)\n",
    "print('Processing %.01f days took %.01f seconds' %(days_p,gw_cluster_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b158a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "plt.bar([\"Download and process: in GeoLab\",\n",
    "        \"S3 and process: in GeoLab\",\"Dask-Gateway in GeoLab\"],\n",
    "        [9,50,440])\n",
    "plt.title(\"Amount of Days of GNSS Data Processed in about 30 seconds\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb8b684",
   "metadata": {},
   "source": [
    "## Takeaways:\n",
    "\n",
    "1. Reading from s3 doesn't require local copies of data flying around\n",
    "    * imagine slow internet\n",
    "    * FAIR data principles\n",
    "3. **Analysis Ready** data doesn't require maintenance of bespoke data format decoders (georinex, obspy/mseed, etc)\n",
    "    * open source libraries built upon mature data containers\n",
    "5. A cloud-hosted notebook compute interface adjacent to the cloud data provides effecient data discovery and exploratory analysis\n",
    "6. A distributed compute infrastructure (dask-gateway) adjacent to ARCO data supports high throughput, massively parallelizable workflows\n",
    "    * large scale data analysis\n",
    "    * Machine learning + deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7069c61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
