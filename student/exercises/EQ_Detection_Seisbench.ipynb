{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35d0e579",
   "metadata": {},
   "source": [
    "# Earthquake Detection Using Seisbench "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50703ed",
   "metadata": {},
   "source": [
    "In recent years, seismology has significantly benefited from machine learning (ML), which enhances efficiency, accuracy, and scope in earthquake detection. ML automates vast data processing, reducing manual effort and enabling real-time monitoring. It improves detection accuracy by recognizing patterns and anomalies often missed by traditional methods. ML models are adaptable and scalable, suitable for various regions and capable of analyzing global seismic data simultaneously. This notebook demonstrates the integration of Seisbench a common package used for ML studies, into an earthquake detection workflow.\n",
    "\n",
    "- [Seisbench Documentation](https://github.com/seisbench/seisbench)\n",
    "- [PhaseNet Documentation](https://github.com/AI4EPS/PhaseNet)\n",
    "- [PyOcto Documentation](https://github.com/yetinam/pyocto)\n",
    "\n",
    "This notebook also leverages **parallel processing**, a method that allows multiple computations to be carried out simultaneously, significantly speeding up data analysis by utilizing multiple CPU cores concurrently.\n",
    "\n",
    "**Author:** Marc Garcia, University of Texas - El Paso, magarcia58@miners.utep.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3a1e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for correct Image (GeoLab)\n",
    "!printenv | grep IMAGE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cc0ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intall necessary packages for notebook\n",
    "!pip install seisbench \n",
    "!pip install pyocto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ecccd4",
   "metadata": {},
   "source": [
    "# 1. Select Region of Interest(Cascadia Subduction Zone)\n",
    "\n",
    "The Cascadia Subduction Zone is a major fault line stretching from northern California to British Columbia, where the Juan de Fuca Plate is subducting beneath the North American Plate. This region is known for producing significant seismic activity, including potential megathrust earthquakes.\n",
    "\n",
    "For this study, we chose the UW (University of Washington) seismic network, which is ideal for testing and demonstrating our machine learning-based earthquake detection workflow. This demo is centered around the recent **magnitude 6.4 earthquake that occurred on July 11, 2024**, approximately 120 miles off Vancouver Island. This earthquake, the largest in a recent series of quakes, highlights the active seismic nature of the Cascadia region.\n",
    "\n",
    "We start by developing a station list by pulling in stations within the defined geographic bounds of the Cascadia Subduction Zone using the ObsPy library. The following code snippet defines the client and geographic bounds, sets the time range for active channels in 2024, and fetches station metadata for the PNSN network (UW) with the desired channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17af714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy.clients.fdsn import Client\n",
    "from obspy import UTCDateTime\n",
    "\n",
    "# Define the client and geographic bounds of the Cascadia Subduction Zone\n",
    "client = Client(\"IRIS\")\n",
    "min_latitude = 40.0\n",
    "max_latitude = 50.0\n",
    "min_longitude = -125.0\n",
    "max_longitude = -120.0\n",
    "\n",
    "# Define the time range for active channels in 2024\n",
    "starttime = UTCDateTime('2024-01-01')\n",
    "endtime = UTCDateTime('2024-12-31')\n",
    "\n",
    "# Fetch station metadata within the defined region for the PNSN network UW, with desired channels\n",
    "inv = client.get_stations(network=\"UW\", station=\"*\", location=\"*\", channel=\"HH*\", \n",
    "                                minlatitude=min_latitude, maxlatitude=max_latitude,\n",
    "                                minlongitude=min_longitude, maxlongitude=max_longitude,\n",
    "                                starttime=starttime, endtime=endtime, level=\"response\")\n",
    "\n",
    "print(inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c853ea67",
   "metadata": {},
   "source": [
    "## Visualizing the stations within the region to understand their distribution and coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb35c800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "# Extract station coordinates\n",
    "stations = [(station.latitude, station.longitude) for network in inv for station in network.stations]\n",
    "\n",
    "# Create a plot\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())\n",
    "\n",
    "# Add geographic features\n",
    "ax.add_feature(cfeature.LAND)\n",
    "ax.add_feature(cfeature.OCEAN)\n",
    "ax.add_feature(cfeature.COASTLINE)\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "\n",
    "# Set the map bounds\n",
    "ax.set_extent([min_longitude, max_longitude, min_latitude, max_latitude], crs=ccrs.PlateCarree())\n",
    "\n",
    "# Plot the stations\n",
    "if stations:\n",
    "    lats, lons = zip(*stations)\n",
    "    ax.scatter(lons, lats, color='red', marker='o', transform=ccrs.PlateCarree(), label='PNSN Stations')\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_title('PNSN Stations in the Cascadia Subduction Zone')\n",
    "ax.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caba638",
   "metadata": {},
   "source": [
    "# 2. Phase Picking with PhaseNet (Seisbench) \n",
    "\n",
    "\n",
    "Using **PhaseNet** with **Seisbench** for phase picking significantly enhances earthquake detection by leveraging machine learning for precise seismic data analysis. PhaseNet, a pre-trained model available in Seisbench, automates the classification of seismic phases (P and S waves), which is crucial for accurate earthquake detection and analysis. The code demonstrates a workflow for using PhaseNet to classify seismic phases within a specified time window, utilizing parallel processing to handle large datasets efficiently.\n",
    "\n",
    "The code begins by setting up necessary imports and initializing the PhaseNet model. It defines a period of interest and extracts a list of stations from the inventory. A function **fetch_waveform_data** is created to fetch waveform data for a given station within a specified time window. The central processing loop iterates through each time window, fetching and combining waveform data from multiple stations concurrently using ThreadPoolExecutor. This parallel processing approach significantly speeds up data retrieval, using multiple CPU cores to handle the high volume of seismic data.\n",
    "\n",
    "Once the waveform data is retrieved, PhaseNet classifies the data to identify P and S wave picks. These picks are then added to a catalog, which is later fed into the associator for further analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f97496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy import UTCDateTime, Stream\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import seisbench.models as sbm\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "picker = sbm.PhaseNet.from_pretrained(\"original\")\n",
    "\n",
    "# Initialize a single catalog for picks\n",
    "pick_list = []\n",
    "\n",
    "# Define start and end times for the period of interest\n",
    "start_time = UTCDateTime(\"2024-07-11T00:00:00\")\n",
    "end_time = UTCDateTime(\"2024-07-13T00:00:00\")\n",
    "window_time = 7200  # Customize windows for fetching data\n",
    "\n",
    "# Extract stations from the inventory\n",
    "station_list = [{\n",
    "    'Network': network.code,\n",
    "    'Station': station.code\n",
    "} for network in inv for station in network.stations]\n",
    "\n",
    "\n",
    "# Function to fetch and filter waveform data for a given station\n",
    "def fetch_waveform_data(station, current_time, window_time):\n",
    "    try:\n",
    "        stream = client.get_waveforms(\n",
    "            network=station['Network'],\n",
    "            station=station['Station'],\n",
    "            location=\"*\",\n",
    "            channel=\"HH?\",\n",
    "            starttime=current_time,\n",
    "            endtime=current_time + window_time\n",
    "        )\n",
    "        if stream:\n",
    "            stream.detrend()  # Detrend before filtering\n",
    "            stream.filter(\"highpass\", freq=5)  # Apply highpass filter at 5 Hz\n",
    "        return stream\n",
    "    except Exception:\n",
    "        return Stream()\n",
    "\n",
    "# Loop through each time window\n",
    "current_time = start_time\n",
    "\n",
    "while current_time < end_time:\n",
    "    combined_stream = Stream()\n",
    "    \n",
    "    # Measure the time taken to download data\n",
    "    start_download_time = time.time()\n",
    "    with ThreadPoolExecutor(max_workers=16) as executor:\n",
    "        futures = [executor.submit(fetch_waveform_data, station, current_time, window_time) for station in station_list]\n",
    "        for future in as_completed(futures):\n",
    "            combined_stream += future.result()\n",
    "    end_download_time = time.time()\n",
    "    \n",
    "    # Classify the combined stream using the PhaseNet picker if it is not empty\n",
    "    if combined_stream:\n",
    "        try:\n",
    "            start_processing_time = time.time()\n",
    "            picks = picker.classify(\n",
    "                combined_stream, \n",
    "                batch_size=256, \n",
    "                P_threshold=0.075, \n",
    "                S_threshold=0.1\n",
    "            ).picks\n",
    "            end_processing_time = time.time()\n",
    "            \n",
    "            # Append picks to the catalog\n",
    "            pick_list.extend([{\n",
    "                \"id\": p.trace_id,\n",
    "                \"timestamp\": p.peak_time.datetime,\n",
    "                \"prob\": p.peak_value,\n",
    "                \"type\": p.phase.lower()\n",
    "            } for p in picks])\n",
    "            \n",
    "            # Print the count of P and S picks and the processing time\n",
    "            pick_count = Counter(p.phase for p in picks)\n",
    "            download_time = end_download_time - start_download_time\n",
    "            processing_time = end_processing_time - start_processing_time\n",
    "            print(f\"Time window starting at {current_time}: {pick_count} (Download time: {download_time:.2f} seconds, Processing time: {processing_time:.2f} seconds)\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing combined stream at {current_time}: {e}\")\n",
    "    \n",
    "    # Move to the next time window\n",
    "    current_time += window_time\n",
    "\n",
    "# Convert the pick catalog to a DataFrame for further analysis or export\n",
    "pick_df = pd.DataFrame(pick_list)\n",
    "\n",
    "# Print the total number of picks\n",
    "print(f\"Total number of picks: {len(pick_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336b1cd9",
   "metadata": {},
   "source": [
    "# 3. Phase Association with PyOcto\n",
    "\n",
    "PyOcto is a high-throughput seismic phase associator that efficiently links seismic phases to specific earthquake events, accurately determining their locations and origin times. It utilizes a 4D space-time partitioning method, starting with a large node that spans the entire study area and iteratively refines potential earthquake locations by splitting nodes based on the highest number of picks. This process is managed using a priority queue system, enhancing processing speed by discarding non-promising nodes early. PyOcto supports homogeneous and 1D velocity models and allows for customizable parameters for multi-region studies. \n",
    "\n",
    "**Note: PyOcto outputs locations, but these are meant to be initial sources. Further refinement of locations is needed for accurate locations.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07ff333",
   "metadata": {},
   "source": [
    "### Associator Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d841451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyocto\n",
    "\n",
    "# Define the velocity model with specified parameters\n",
    "velocity_model = pyocto.VelocityModel0D(\n",
    "    p_velocity=7.0,\n",
    "    s_velocity=4.0,\n",
    "    tolerance=2.0,\n",
    "    association_cutoff_distance=300,\n",
    ")\n",
    "\n",
    "# Create the associator with corrected latitude and longitude ranges\n",
    "associator = pyocto.OctoAssociator.from_area(\n",
    "    lat=(40, 55),  \n",
    "    lon=(-135, -120),  \n",
    "    zlim=(0, 200),\n",
    "    time_before=300,\n",
    "    velocity_model=velocity_model,\n",
    "    n_picks=6,\n",
    "    n_p_and_s_picks=3,\n",
    ")\n",
    "\n",
    "# Convert station information to the required format\n",
    "stations = associator.inventory_to_df(inv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdcc3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_df = stations\n",
    "stations_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f827e28",
   "metadata": {},
   "source": [
    "### Association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220125aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the phase association \n",
    "events, assignments = associator.associate_seisbench(picks, stations)\n",
    "\n",
    "# Transform event coordinates back to latitude and longitude\n",
    "associator.transform_events(events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67979ea6",
   "metadata": {},
   "source": [
    "# 4. Visualize the Results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd958bc",
   "metadata": {},
   "source": [
    "## Map View of Located Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f5a675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "min_latitude = 41.0\n",
    "max_latitude = 54.0\n",
    "min_longitude = -132.0\n",
    "max_longitude = -118.0\n",
    "\n",
    "# Extract station coordinates\n",
    "stations = [(station.latitude, station.longitude) for network in inv for station in network.stations]\n",
    "\n",
    "# Extract event coordinates\n",
    "event_lats = events['latitude'].tolist()\n",
    "event_lons = events['longitude'].tolist()\n",
    "\n",
    "# Create a plot\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())\n",
    "\n",
    "# Add geographic features\n",
    "ax.add_feature(cfeature.LAND)\n",
    "ax.add_feature(cfeature.OCEAN)\n",
    "ax.add_feature(cfeature.COASTLINE)\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "\n",
    "# Set the map bounds\n",
    "ax.set_extent([min_longitude, max_longitude, min_latitude, max_latitude], crs=ccrs.PlateCarree())\n",
    "\n",
    "# Plot the stations\n",
    "if stations:\n",
    "    lats, lons = zip(*stations)\n",
    "    ax.scatter(lons, lats, color='red', marker='o',s=10, transform=ccrs.PlateCarree(), label='PNSN Stations')\n",
    "\n",
    "# Plot the earthquake events\n",
    "ax.scatter(event_lons, event_lats, color='blue', marker='x',s=12, transform=ccrs.PlateCarree(), label='Earthquake Events')\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_title('Earthquake Events in the Cascadia Subduction Zone')\n",
    "ax.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6090bf79",
   "metadata": {},
   "source": [
    "## Inspect Some Events (Waveforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca6eb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d04d899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import obspy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the event and picks as per the initial code\n",
    "event_idx = np.random.choice(events[\"idx\"])\n",
    "event_picks = [picks[i] for i in assignments[assignments[\"event_idx\"] == event_idx][\"pick_idx\"]]\n",
    "event = events[events[\"idx\"] == event_idx].iloc[0]\n",
    "\n",
    "station_dict = {station: (x, y) for station, x, y in zip(stations_df[\"id\"], stations_df[\"x\"], stations_df[\"y\"])}\n",
    "\n",
    "first, last = min(pick.peak_time for pick in event_picks), max(pick.peak_time for pick in event_picks)\n",
    "\n",
    "sub = obspy.Stream()\n",
    "\n",
    "for station in np.unique([pick.trace_id for pick in event_picks]):\n",
    "    sub.append(combined_stream.select(station=station.split('.')[1], channel=\"HHZ\")[0])\n",
    "\n",
    "sub = sub.slice(first - 5, last + 5)\n",
    "\n",
    "sub = sub.copy()\n",
    "sub.detrend()\n",
    "sub.filter(\"highpass\", freq=5)\n",
    "\n",
    "fig, axs = plt.subplots(len(sub), 1, figsize=(10, len(sub) * 1.5), sharex=True)\n",
    "\n",
    "if len(sub) == 1:\n",
    "    axs = [axs]\n",
    "\n",
    "for i, trace in enumerate(sub):\n",
    "    normed = trace.data - np.mean(trace.data)\n",
    "    normed = normed / np.max(np.abs(normed))\n",
    "    station_x, station_y = station_dict[trace.id.split('.')[1]]\n",
    "    y = np.sqrt((station_x - event[\"x\"]) ** 2 + (station_y - event[\"y\"]) ** 2 + event[\"z\"] ** 2)\n",
    "    \n",
    "    axs[i].plot(trace.times(), normed, color='royalblue')  # Set waveform color to dark gray\n",
    "    axs[i].set_xlim(0, trace.times()[-1])  # Adjust x-axis limits to trace start and end times\n",
    "    axs[i].set_ylabel(f\"{y:.2f} km\")\n",
    "    axs[i].yaxis.set_ticks([])  # Remove y-axis ticks\n",
    "    axs[i].tick_params(axis='y', which='both', length=0)  # Remove y-axis tick marks\n",
    "    \n",
    "    for pick in event_picks:\n",
    "        if pick.trace_id.split('.')[1] == trace.id.split('.')[1]:\n",
    "            x = pick.peak_time - trace.stats.starttime\n",
    "            if pick.phase == \"P\":\n",
    "                ls = '-'\n",
    "            else:\n",
    "                ls = '--'\n",
    "            axs[i].axvline(x=x, color='k', linestyle=ls)\n",
    "\n",
    "axs[-1].set_xlabel(\"Time [s]\")\n",
    "\n",
    "# Manually adjust the spacing between subplots\n",
    "plt.subplots_adjust(hspace=0)\n",
    "plt.show()\n",
    "\n",
    "print(\"Event information\")\n",
    "print(event)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c814365",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
